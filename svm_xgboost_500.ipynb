{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn.ensemble\n",
    "from sklearn.metrics import confusion_matrix,classification_report,accuracy_score\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split,StratifiedKFold\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import lime\n",
    "from lime import lime_text\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "from sklearn.pipeline import make_pipeline,Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "import joblib\n",
    "from nltk import word_tokenize\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import html,nltk\n",
    "from nltk.corpus import wordnet \n",
    "from collections import Counter \n",
    "from string import digits\n",
    "\n",
    "def text_cleaning(text, escape_list=[], stop=[]):\n",
    "    \"\"\"\n",
    "    Text cleaning function:\n",
    "    \"\"\"\n",
    "    text=text.lower()\n",
    "    StopWords = list(set(stopwords.words('dutch')))\n",
    "    custom_stop = StopWords + stop\n",
    "    text = html.unescape(text)\n",
    "    text = re.sub('[^A-Za-z]+', ' ', text)\n",
    "    text=text.replace('/',' ').replace('?',' ').replace(',',' ').replace('\\'',' ')\n",
    "    tokens=nltk.word_tokenize(text)\n",
    "    tokens=([token for token in tokens if token not in custom_stop]) \n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/court_cases.csv\", lineterminator='\\n', index_col=0)\n",
    "df['Full Text'] = df['process'] + ' ' + df['considerations']\n",
    "df.dropna(subset=['Full Text'],inplace=True)\n",
    "df = df.sample(frac=1).reset_index(drop=True).copy()\n",
    "df['Full Text'] = df['Full Text'].apply(text_cleaning)\n",
    "df['Full Text'] = df['Full Text'].apply(lambda x: ' '.join(word_tokenize(x)[-500:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rus = RandomUnderSampler(random_state=42)\n",
    "X= df[['process', 'considerations', 'instance','Full Text']]\n",
    "y = df[['outcome']]\n",
    "X_rus, y_rus = rus.fit_resample(X,y)\n",
    "X_rus['outcome'] = y_rus\n",
    "df = X_rus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = df_full['Full Text'], df_full.outcome\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y , test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'tfidf__max_df': (0.25, 0.5, 0.75),\n",
    "    'svm__C': [0.001,0.01,0.1,1,10,100,1000],\n",
    "}\n",
    "\n",
    "tfidf = TfidfVectorizer(sublinear_tf=True, norm='l2', ngram_range= (1,1))\n",
    "svm = LinearSVC(random_state=42)\n",
    "pipeline = Pipeline(steps = [('tfidf',tfidf),('svm',svm)])\n",
    "skf_cv = StratifiedKFold(n_splits = 5, random_state = 42)\n",
    "rsc = RandomizedSearchCV(pipeline,params,c v= skf_cv,scoring='accuracy', random_state=42,n_jobs=-1)\n",
    "rsc.fit(X,y)\n",
    "print(f'Best CrossValidated accuracy achieved via SVM is : {round(rsc.best_score_*100,2)} %')\n",
    "# Best params for SVM is: \n",
    "rsc.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best params for SVM is: \n",
    "rsc.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = rsc.best_estimator_.named_steps[\"tfidf\"].get_feature_names()\n",
    "feature_importance = rsc.best_estimator_.named_steps[\"svm\"].coef_.flatten()\n",
    "fi = pd.DataFrame({'FeatureNames':feature_names,'FeatureImportance':feature_importance}).sort_values('FeatureImportance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fi['color'] = fi.FeatureImportance.apply(lambda x:'Positive' if x>=0 else 'Negative')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_features_of_explained_model(imp):\n",
    "    imp = imp.copy()\n",
    "    imp.set_index('FeatureNames',inplace=True)\n",
    "    imp['color'] = imp['FeatureImportance'].apply(lambda x:'Positive' if x>=0 else 'Negative')\n",
    "    imp['FeatureImportance'] = imp['FeatureImportance'].apply(abs)\n",
    "    imp = imp.sort_values('FeatureImportance',ascending=False)[:50]\n",
    "    sns.set(rc={'figure.figsize':(12,12)})\n",
    "    palette = [\"#55a868\",\"#c44e52\"]\n",
    "    ax = sns.barplot(x=imp['FeatureImportance'], y=imp.index,hue=imp['color'],palette=palette, dodge=False,hue_order=[\"Positive\", \"Negative\"]).set_title(f'Feature Importance :',fontsize=15)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_features_of_explained_model(fi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_svm = Pipeline(steps = [('tfidf',TfidfVectorizer(sublinear_tf=True, norm='l2',ngram_range=(1,1),max_df=.5)),\n",
    "                        ('svm',CalibratedClassifierCV(LinearSVC(C=.1),method='isotonic'))])\n",
    "\n",
    "pipeline_svm.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the features\n",
    "fi.FeatureImportance = fi.FeatureImportance.apply(abs)\n",
    "fi.sort_values('FeatureImportance',ascending=False).to_csv('../features/svm_1_1_builtin_500_words.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_importance_for_full_document_parallel(X,y,idx,pipeline,num_features):\n",
    "\n",
    "    explainer = LimeTextExplainer(class_names=[0,1])\n",
    "    exp = explainer.explain_instance(X, pipeline.predict_proba, num_features=num_features, labels=[0,1])\n",
    "    imp = pd.DataFrame(exp.as_list(label=1),columns=['word','importance'])\n",
    "    imp['ID'] = idx\n",
    "    imp['Real Class'] = y\n",
    "    imp['Predicted Class'] = pipeline.predict([X]).reshape(1,-1)[0,0]\n",
    "    imp['Predicted Class Probability'] = pipeline.predict_proba([X]).max()\n",
    "    return imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to speed up lime with multiprocessing\n",
    "\n",
    "import multiprocessing\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "def get_line_fi_multiprocessing(X,y,pipeline):\n",
    "    start = time.time()\n",
    "\n",
    "    with multiprocessing.Pool(processes=1) as pool:\n",
    "        results = pool.starmap(get_feature_importance_for_full_document_parallel, [(X[idx],y[idx],idx,pipeline,500) for idx in range(len(X))])\n",
    "        \n",
    "    full_df = pd.concat(results)\n",
    "    end = time.time()\n",
    "\n",
    "    total_time = end - start\n",
    "    print('Time :',total_time)\n",
    "    print(f'With multiprocessing the job was finished in {int(total_time/3600)} hours {int(round(((total_time/3600)%1)*60,0))} minutes.')\n",
    "    \n",
    "    return full_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lime_svm_fi = get_line_fi_multiprocessing(X,y,pipeline_svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lime_svm_fi.to_csv('../features/svm_1_1_lime_500_words.csv.gz',index=False,compression='gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = df['Full Text'], df.outcome\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y , test_size=0.3, random_state=3)\n",
    "\n",
    "def get_shap_values_full(pipeline_svm,X_test,classifier_name):\n",
    "    X_tfidf_tfidf = pipeline_svm.named_steps['tfidf'].transform(X_test)\n",
    "    X_train_summary = shap.kmeans(X_tfidf_tfidf, 10)\n",
    "    explainer = shap.KernelExplainer(pipeline_svm.named_steps[classifier_name].predict_proba, X_train_summary)\n",
    "    tfidf_features_svm = pipeline_svm.named_steps['tfidf'].get_feature_names()\n",
    "    per_loop_count = 20\n",
    "    start, end = 0, per_loop_count\n",
    "    final_df_svm=pd.DataFrame()\n",
    "    while end < len(X_test):\n",
    "        print(end)\n",
    "        X_tfidf = X_tfidf_tfidf[start:end].copy()\n",
    "        loop_df = get_shap_feature_importance(explainer, X_tfidf,tfidf_features_svm)\n",
    "        final_df_svm = pd.concat([loop_df,final_df_svm])\n",
    "        start+=per_loop_count\n",
    "        end+=per_loop_count\n",
    "        if end%50:\n",
    "            final_df_svm = final_df_svm.query('importance!=0')\n",
    "    end = len(X_test)\n",
    "    X_tfidf = X_tfidf_tfidf[start:end].copy()\n",
    "    loop_df = get_shap_feature_importance(explainer, X_tfidf,tfidf_features_svm)\n",
    "    final_df_svm = pd.concat([loop_df,final_df_svm])\n",
    "    final_df_svm = final_df_svm.query('importance!=0')\n",
    "    return final_df_svm\n",
    "\n",
    "def get_shap_feature_importance(explainer, X_tfidf,tfidf_features_svm):\n",
    "    loop_shap_values = explainer.shap_values(X_tfidf)\n",
    "    loop_vals= np.abs(loop_shap_values).mean(0)\n",
    "    loop_feature_importance_shap_svm = pd.DataFrame(list(zip(tfidf_features_svm, sum(loop_vals))), columns=['word','importance'])\n",
    "    return loop_feature_importance_shap_svm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df_svm = get_shap_values_full(pipeline_svm,X_test,'svm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df_svm.to_csv('../features/svm_1_1_shap_500_words.csv.gz',index=False,compression='gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training XGBOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "params_xgb = {\n",
    "    'tfidf__max_df': (0.25, 0.5, 0.75),\n",
    "    'xgb__eta': [0.01,0.015,0.025,0.05, 0.1],\n",
    "    'xgb__gamma':[0.05,.1,.3,.5,.7,.9,1],\n",
    "    'xgb__max_depth' : [3,5,7,9,12,15,17,25,50,100],\n",
    "    'xgb__min_child_weight' : [1,3,5,7],\n",
    "    'xgb__subsample' : [0.6,.7,.8,.9,1],\n",
    "    'xgb__colsample_bytree' : [.6,.7,.8,.9,1],\n",
    "    'xgb__lambda' : [0.01,.1,1],\n",
    "    'xgb__alpha': [0,.1,.5,1]\n",
    "}\n",
    "\n",
    "tfidf = TfidfVectorizer(sublinear_tf=True, norm='l2',ngram_range=(1,1))\n",
    "xgb = XGBClassifier(TREE_METHOD = 'gpu_hist', random_state=42)\n",
    "pipeline_xgb = Pipeline(steps = [('tfidf',tfidf),('xgb',xgb)])\n",
    "skf_cv = StratifiedKFold(n_splits=5, random_state = 42)\n",
    "rsc_xgb = RandomizedSearchCV(pipeline_xgb,params_xgb,cv= skf_cv,scoring='accuracy', random_state=0,n_jobs=-1)\n",
    "rsc_xgb.fit(X,y)\n",
    "print(f'Best CrossValidated accuracy achieved via SVM is : {round(rsc_xgb.best_score_*100,2)} %\\n')\n",
    "from pprint import pprint\n",
    "print('XGB Best params : \\n')\n",
    "pprint(rsc_xgb.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_xgb = rsc_xgb.best_estimator_.named_steps[\"xgb\"].get_booster().get_score(importance_type = \"gain\")\n",
    "vocab = rsc_xgb.best_estimator_.named_steps[\"tfidf\"].vocabulary_\n",
    "key_to_vocab = dict([(value, key) for key, value in vocab.items()]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance_dict_xgb = {key_to_vocab[int(key[1:])] : value for key,value in feature_xgb.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance_df_xgb = pd.DataFrame(feature_importance_dict_xgb,columns=['Features','Importance'],index=range(len(feature_importance_dict_xgb)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_feature_importance = pd.DataFrame(feature_importance_dict_xgb,index=range(len(feature_importance_dict_xgb))).T.iloc[:,0].to_frame('importance')\n",
    "xgb_feature_importance.sort_values('importance',ascending=False,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_feature_importance = xgb_feature_importance.iloc[:50,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(x=xgb_feature_importance['importance'], y=xgb_feature_importance.index,palette=[\"#55a868\"], dodge=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_feature_importance.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_feature_importance.columns = ['FeatureNames', 'FeatureImportance']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_feature_importance.to_csv('../features/xgb_1_1_builtin_500_words.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rsc_xgb.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_xgb = Pipeline(steps = [('tfidf',TfidfVectorizer(sublinear_tf=True, norm='l2',ngram_range=(1,1),max_df=.5)),\n",
    "                        ('xgb', XGBClassifier(subsample=.7,min_child_weight=3,max_depth=17,gamma=.3,eta=.01,colsample_bytree=.6,alpha=1))])\n",
    "\n",
    "pipeline_xgb.fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "xgb_fis = []\n",
    "for idx in tqdm(range(len(X))):\n",
    "    loop_fi = get_feature_importance_for_full_document_parallel(X[idx],y[idx],idx,pipeline_xgb,500)\n",
    "    xgb_fis.append(loop_fi)\n",
    "lime_xgb_fi = pd.concat(xgb_fis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lime_xgb_fi.to_csv('../features/xgb_1_1_lime_500_words.csv.gz',index=False,compression='gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df_xgb = get_shap_values_full(pipeline_xgb,X_test,'xgb')\n",
    "final_df_xgb.to_csv('../features/xgb_1_1_shap_500_words.csv.gz',index=False,compression='gzip')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
