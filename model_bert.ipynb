{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install simpletransformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import collections\n",
    "import logging\n",
    "import os\n",
    "import unicodedata\n",
    "from io import open\n",
    "\n",
    "def load_vocab(vocab_file):\n",
    "    \"\"\"Loads a vocabulary file into a dictionary.\"\"\"\n",
    "    vocab = collections.OrderedDict()\n",
    "    index = 0\n",
    "    with open(vocab_file, \"r\", encoding=\"utf-8\") as reader:\n",
    "        while True:\n",
    "            token = reader.readline()\n",
    "            if not token:\n",
    "                break\n",
    "            token = token.strip()\n",
    "            vocab[token] = index\n",
    "            index += 1\n",
    "    return vocab\n",
    "\n",
    "\n",
    "def whitespace_tokenize(text):\n",
    "    \"\"\"Runs basic whitespace cleaning and splitting on a piece of text.\"\"\"\n",
    "    text = text.strip()\n",
    "    if not text:\n",
    "        return []\n",
    "    tokens = text.split()\n",
    "    return tokens\n",
    "\n",
    "\n",
    "class BertTokenizer(object):\n",
    "    \"\"\"Runs end-to-end tokenization: punctuation splitting + wordpiece\"\"\"\n",
    "\n",
    "    def __init__(self, vocab_file, do_lower_case=True, max_len=None, do_basic_tokenize=True,\n",
    "                 never_split=(\"[UNK]\", \"[SEP]\", \"[PAD]\", \"[CLS]\", \"[MASK]\")):\n",
    "        \"\"\"Constructs a BertTokenizer.\n",
    "\n",
    "        Args:\n",
    "          vocab_file: Path to a one-wordpiece-per-line vocabulary file\n",
    "          do_lower_case: Whether to lower case the input\n",
    "                         Only has an effect when do_wordpiece_only=False\n",
    "          do_basic_tokenize: Whether to do basic tokenization before wordpiece.\n",
    "          max_len: An artificial maximum length to truncate tokenized sequences to;\n",
    "                         Effective maximum length is always the minimum of this\n",
    "                         value (if specified) and the underlying BERT model's\n",
    "                         sequence length.\n",
    "          never_split: List of tokens which will never be split during tokenization.\n",
    "                         Only has an effect when do_wordpiece_only=False\n",
    "        \"\"\"\n",
    "        if not os.path.isfile(vocab_file):\n",
    "            raise ValueError(\n",
    "                \"Can't find a vocabulary file at path '{}'. To load the vocabulary from a Google pretrained \"\n",
    "                \"model use `tokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)`\".format(vocab_file))\n",
    "        self.vocab = load_vocab(vocab_file)\n",
    "        self.ids_to_tokens = collections.OrderedDict(\n",
    "            [(ids, tok) for tok, ids in self.vocab.items()])\n",
    "        self.do_basic_tokenize = do_basic_tokenize\n",
    "        if do_basic_tokenize:\n",
    "            self.basic_tokenizer = BasicTokenizer(do_lower_case=do_lower_case,\n",
    "                                                never_split=never_split)\n",
    "        self.wordpiece_tokenizer = WordpieceTokenizer(vocab=self.vocab)\n",
    "        self.max_len = max_len if max_len is not None else int(1e12)\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        split_tokens = []\n",
    "        if self.do_basic_tokenize:\n",
    "            for token in self.basic_tokenizer.tokenize(text):\n",
    "                for sub_token in self.wordpiece_tokenizer.tokenize(token):\n",
    "                    split_tokens.append(sub_token)\n",
    "        else:\n",
    "            split_tokens = self.wordpiece_tokenizer.tokenize(text)\n",
    "        return split_tokens\n",
    "\n",
    "    def convert_tokens_to_ids(self, tokens):\n",
    "        \"\"\"Converts a sequence of tokens into ids using the vocab.\"\"\"\n",
    "        ids = []\n",
    "        for token in tokens:\n",
    "            ids.append(self.vocab[token])\n",
    "        if len(ids) > self.max_len:\n",
    "            logger.warning(\n",
    "                \"Token indices sequence length is longer than the specified maximum \"\n",
    "                \" sequence length for this BERT model ({} > {}). Running this\"\n",
    "                \" sequence through BERT will result in indexing errors\".format(len(ids), self.max_len)\n",
    "            )\n",
    "        return ids\n",
    "\n",
    "    def convert_ids_to_tokens(self, ids):\n",
    "        \"\"\"Converts a sequence of ids in wordpiece tokens using the vocab.\"\"\"\n",
    "        tokens = []\n",
    "        for i in ids:\n",
    "            tokens.append(self.ids_to_tokens[i])\n",
    "        return tokens\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, pretrained_model_name_or_path, cache_dir=None, *inputs, **kwargs):\n",
    "        \"\"\"\n",
    "        Instantiate a PreTrainedBertModel from a pre-trained model file.\n",
    "        Download and cache the pre-trained model file if needed.\n",
    "        \"\"\"\n",
    "        if pretrained_model_name_or_path in PRETRAINED_VOCAB_ARCHIVE_MAP:\n",
    "            vocab_file = PRETRAINED_VOCAB_ARCHIVE_MAP[pretrained_model_name_or_path]\n",
    "            if '-cased' in pretrained_model_name_or_path and kwargs.get('do_lower_case', True):\n",
    "                logger.warning(\"The pre-trained model you are loading is a cased model but you have not set \"\n",
    "                               \"`do_lower_case` to False. We are setting `do_lower_case=False` for you but \"\n",
    "                               \"you may want to check this behavior.\")\n",
    "                kwargs['do_lower_case'] = False\n",
    "            elif '-cased' not in pretrained_model_name_or_path and not kwargs.get('do_lower_case', True):\n",
    "                logger.warning(\"The pre-trained model you are loading is an uncased model but you have set \"\n",
    "                               \"`do_lower_case` to False. We are setting `do_lower_case=True` for you \"\n",
    "                               \"but you may want to check this behavior.\")\n",
    "                kwargs['do_lower_case'] = True\n",
    "        else:\n",
    "            vocab_file = pretrained_model_name_or_path\n",
    "        if os.path.isdir(vocab_file):\n",
    "            vocab_file = os.path.join(vocab_file, VOCAB_NAME)\n",
    "        # redirect to the cache, if necessary\n",
    "        try:\n",
    "            resolved_vocab_file = cached_path(vocab_file, cache_dir=cache_dir)\n",
    "        except EnvironmentError:\n",
    "            logger.error(\n",
    "                \"Model name '{}' was not found in model name list ({}). \"\n",
    "                \"We assumed '{}' was a path or url but couldn't find any file \"\n",
    "                \"associated to this path or url.\".format(\n",
    "                    pretrained_model_name_or_path,\n",
    "                    ', '.join(PRETRAINED_VOCAB_ARCHIVE_MAP.keys()),\n",
    "                    vocab_file))\n",
    "            return None\n",
    "        if resolved_vocab_file == vocab_file:\n",
    "            logger.info(\"loading vocabulary file {}\".format(vocab_file))\n",
    "        else:\n",
    "            logger.info(\"loading vocabulary file {} from cache at {}\".format(\n",
    "                vocab_file, resolved_vocab_file))\n",
    "        if pretrained_model_name_or_path in PRETRAINED_VOCAB_POSITIONAL_EMBEDDINGS_SIZE_MAP:\n",
    "            # if we're using a pretrained model, ensure the tokenizer wont index sequences longer\n",
    "            # than the number of positional embeddings\n",
    "            max_len = PRETRAINED_VOCAB_POSITIONAL_EMBEDDINGS_SIZE_MAP[pretrained_model_name_or_path]\n",
    "            kwargs['max_len'] = min(kwargs.get('max_len', int(1e12)), max_len)\n",
    "        # Instantiate tokenizer.\n",
    "        tokenizer = cls(resolved_vocab_file, *inputs, **kwargs)\n",
    "        return tokenizer\n",
    "    \n",
    "    \n",
    "class BasicTokenizer(object):\n",
    "    \"\"\"Runs basic tokenization (punctuation splitting, lower casing, etc.).\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 do_lower_case=True,\n",
    "                 never_split=(\"[UNK]\", \"[SEP]\", \"[PAD]\", \"[CLS]\", \"[MASK]\")):\n",
    "        \"\"\"Constructs a BasicTokenizer.\n",
    "\n",
    "        Args:\n",
    "          do_lower_case: Whether to lower case the input.\n",
    "        \"\"\"\n",
    "        self.do_lower_case = do_lower_case\n",
    "        self.never_split = never_split\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        \"\"\"Tokenizes a piece of text.\"\"\"\n",
    "        text = self._clean_text(text)\n",
    "        # This was added on November 1st, 2018 for the multilingual and Chinese\n",
    "        # models. This is also applied to the English models now, but it doesn't\n",
    "        # matter since the English models were not trained on any Chinese data\n",
    "        # and generally don't have any Chinese data in them (there are Chinese\n",
    "        # characters in the vocabulary because Wikipedia does have some Chinese\n",
    "        # words in the English Wikipedia.).\n",
    "        text = self._tokenize_chinese_chars(text)\n",
    "        orig_tokens = whitespace_tokenize(text)\n",
    "        split_tokens = []\n",
    "        for token in orig_tokens:\n",
    "            if self.do_lower_case and token not in self.never_split:\n",
    "                token = token.lower()\n",
    "                token = self._run_strip_accents(token)\n",
    "            split_tokens.extend(self._run_split_on_punc(token))\n",
    "\n",
    "        output_tokens = whitespace_tokenize(\" \".join(split_tokens))\n",
    "        return output_tokens\n",
    "\n",
    "    def _run_strip_accents(self, text):\n",
    "        \"\"\"Strips accents from a piece of text.\"\"\"\n",
    "        text = unicodedata.normalize(\"NFD\", text)\n",
    "        output = []\n",
    "        for char in text:\n",
    "            cat = unicodedata.category(char)\n",
    "            if cat == \"Mn\":\n",
    "                continue\n",
    "            output.append(char)\n",
    "        return \"\".join(output)\n",
    "\n",
    "    def _run_split_on_punc(self, text):\n",
    "        \"\"\"Splits punctuation on a piece of text.\"\"\"\n",
    "        if text in self.never_split:\n",
    "            return [text]\n",
    "        chars = list(text)\n",
    "        i = 0\n",
    "        start_new_word = True\n",
    "        output = []\n",
    "        while i < len(chars):\n",
    "            char = chars[i]\n",
    "            if _is_punctuation(char):\n",
    "                output.append([char])\n",
    "                start_new_word = True\n",
    "            else:\n",
    "                if start_new_word:\n",
    "                    output.append([])\n",
    "                start_new_word = False\n",
    "                output[-1].append(char)\n",
    "            i += 1\n",
    "\n",
    "        return [\"\".join(x) for x in output]\n",
    "\n",
    "    def _tokenize_chinese_chars(self, text):\n",
    "        \"\"\"Adds whitespace around any CJK character.\"\"\"\n",
    "        output = []\n",
    "        for char in text:\n",
    "            cp = ord(char)\n",
    "            if self._is_chinese_char(cp):\n",
    "                output.append(\" \")\n",
    "                output.append(char)\n",
    "                output.append(\" \")\n",
    "            else:\n",
    "                output.append(char)\n",
    "        return \"\".join(output)\n",
    "\n",
    "    def _is_chinese_char(self, cp):\n",
    "        \"\"\"Checks whether CP is the codepoint of a CJK character.\"\"\"\n",
    "        # This defines a \"chinese character\" as anything in the CJK Unicode block:\n",
    "        #   https://en.wikipedia.org/wiki/CJK_Unified_Ideographs_(Unicode_block)\n",
    "        #\n",
    "        # Note that the CJK Unicode block is NOT all Japanese and Korean characters,\n",
    "        # despite its name. The modern Korean Hangul alphabet is a different block,\n",
    "        # as is Japanese Hiragana and Katakana. Those alphabets are used to write\n",
    "        # space-separated words, so they are not treated specially and handled\n",
    "        # like the all of the other languages.\n",
    "        if ((cp >= 0x4E00 and cp <= 0x9FFF) or  #\n",
    "                (cp >= 0x3400 and cp <= 0x4DBF) or  #\n",
    "                (cp >= 0x20000 and cp <= 0x2A6DF) or  #\n",
    "                (cp >= 0x2A700 and cp <= 0x2B73F) or  #\n",
    "                (cp >= 0x2B740 and cp <= 0x2B81F) or  #\n",
    "                (cp >= 0x2B820 and cp <= 0x2CEAF) or\n",
    "                (cp >= 0xF900 and cp <= 0xFAFF) or  #\n",
    "                (cp >= 0x2F800 and cp <= 0x2FA1F)):  #\n",
    "            return True\n",
    "\n",
    "        return False\n",
    "\n",
    "    def _clean_text(self, text):\n",
    "        \"\"\"Performs invalid character removal and whitespace cleanup on text.\"\"\"\n",
    "        output = []\n",
    "        for char in text:\n",
    "            cp = ord(char)\n",
    "            if cp == 0 or cp == 0xfffd or _is_control(char):\n",
    "                continue\n",
    "            if _is_whitespace(char):\n",
    "                output.append(\" \")\n",
    "            else:\n",
    "                output.append(char)\n",
    "        return \"\".join(output)\n",
    "\n",
    "\n",
    "class WordpieceTokenizer(object):\n",
    "    \"\"\"Runs WordPiece tokenization.\"\"\"\n",
    "\n",
    "    def __init__(self, vocab, unk_token=\"[UNK]\", max_input_chars_per_word=100):\n",
    "        self.vocab = vocab\n",
    "        self.unk_token = unk_token\n",
    "        self.max_input_chars_per_word = max_input_chars_per_word\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        \"\"\"Tokenizes a piece of text into its word pieces.\n",
    "\n",
    "        This uses a greedy longest-match-first algorithm to perform tokenization\n",
    "        using the given vocabulary.\n",
    "\n",
    "        For example:\n",
    "          input = \"unaffable\"\n",
    "          output = [\"un\", \"##aff\", \"##able\"]\n",
    "\n",
    "        Args:\n",
    "          text: A single token or whitespace separated tokens. This should have\n",
    "            already been passed through `BasicTokenizer`.\n",
    "\n",
    "        Returns:\n",
    "          A list of wordpiece tokens.\n",
    "        \"\"\"\n",
    "\n",
    "        output_tokens = []\n",
    "        for token in whitespace_tokenize(text):\n",
    "            chars = list(token)\n",
    "            if len(chars) > self.max_input_chars_per_word:\n",
    "                output_tokens.append(self.unk_token)\n",
    "                continue\n",
    "\n",
    "            is_bad = False\n",
    "            start = 0\n",
    "            sub_tokens = []\n",
    "            while start < len(chars):\n",
    "                end = len(chars)\n",
    "                cur_substr = None\n",
    "                while start < end:\n",
    "                    substr = \"\".join(chars[start:end])\n",
    "                    if start > 0:\n",
    "                        substr = \"##\" + substr\n",
    "                    if substr in self.vocab:\n",
    "                        cur_substr = substr\n",
    "                        break\n",
    "                    end -= 1\n",
    "                if cur_substr is None:\n",
    "                    is_bad = True\n",
    "                    break\n",
    "                sub_tokens.append(cur_substr)\n",
    "                start = end\n",
    "\n",
    "            if is_bad:\n",
    "                output_tokens.append(self.unk_token)\n",
    "            else:\n",
    "                output_tokens.extend(sub_tokens)\n",
    "        return output_tokens\n",
    "\n",
    "\n",
    "def _is_whitespace(char):\n",
    "    \"\"\"Checks whether `chars` is a whitespace character.\"\"\"\n",
    "    # \\t, \\n, and \\r are technically contorl characters but we treat them\n",
    "    # as whitespace since they are generally considered as such.\n",
    "    if char == \" \" or char == \"\\t\" or char == \"\\n\" or char == \"\\r\":\n",
    "        return True\n",
    "    cat = unicodedata.category(char)\n",
    "    if cat == \"Zs\":\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def _is_control(char):\n",
    "    \"\"\"Checks whether `chars` is a control character.\"\"\"\n",
    "    # These are technically control characters but we count them as whitespace\n",
    "    # characters.\n",
    "    if char == \"\\t\" or char == \"\\n\" or char == \"\\r\":\n",
    "        return False\n",
    "    cat = unicodedata.category(char)\n",
    "    if cat.startswith(\"C\"):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def _is_punctuation(char):\n",
    "    \"\"\"Checks whether `chars` is a punctuation character.\"\"\"\n",
    "    cp = ord(char)\n",
    "    # We treat all non-letter/number ASCII as punctuation.\n",
    "    # Characters such as \"^\", \"$\", and \"`\" are not in the Unicode\n",
    "    # Punctuation class but we treat them as punctuation anyways, for\n",
    "    # consistency.\n",
    "    if ((cp >= 33 and cp <= 47) or (cp >= 58 and cp <= 64) or\n",
    "            (cp >= 91 and cp <= 96) or (cp >= 123 and cp <= 126)):\n",
    "        return True\n",
    "    cat = unicodedata.category(char)\n",
    "    if cat.startswith(\"P\"):\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export CUDA_DEVICE_ORDER=\"PCI_BUS_ID\"\n",
    "# export CUDA_VISIBLE_DEVICES=\"5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"5\" # for single gpu   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import html,nltk\n",
    "from nltk import sent_tokenize,word_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.corpus import wordnet \n",
    "from collections import Counter \n",
    "from string import digits\n",
    "from imblearn.under_sampling import RandomUnderSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/court_cases.csv\", lineterminator='\\n', index_col=0)\n",
    "df['Full Text'] = df['process'] + ' ' + df['considerations']\n",
    "df.dropna(subset=['Full Text'],inplace=True)\n",
    "df = df.sample(frac=1,random_state=0).reset_index(drop=True).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rus = RandomUnderSampler(random_state=42)\n",
    "X= df[['process', 'considerations', 'instance','Full Text']]\n",
    "y = df[['outcome']]\n",
    "X_rus, y_rus = rus.fit_resample(X,y)\n",
    "X_rus['outcome'] = y_rus\n",
    "df = X_rus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_aligned(real,bert_tokenized):\n",
    "    '''\n",
    "    Align the output\n",
    "    '''\n",
    "    bert_tokenized = sent_tokenize(re.sub(r'##\\w+ ', '', bert_tokenized))\n",
    "    length_bert_tokenized = len(bert_tokenized)\n",
    "    real = sent_tokenize(real)[-length_bert_tokenized:]\n",
    "    length_first_sentence = len(bert_tokenized[0].split(' ') )\n",
    "    real[0] =' '.join(word_tokenize(real[0])[-length_first_sentence:])\n",
    "    return ' '.join( word_tokenize(' '.join(real)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer('outputs/vocab.txt')\n",
    "df['Full Text'] = df['Full Text'].apply(lambda x : ' '.join( tokenizer.tokenize(x)[-500:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bert_data = df[['Full Text','outcome']].copy()\n",
    "df_bert_data.columns = ['sent','label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(df_bert_data , test_size=0.3, random_state=42)\n",
    "SAVE_ROOT_PATH = '../feature-importance/data/court_cases/'\n",
    "train.to_csv(SAVE_ROOT_PATH+'train.tsv',sep='\\t',index=False)\n",
    "test.to_csv(SAVE_ROOT_PATH+'test.tsv',sep='\\t',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_real = df[test.index,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aligned_text_list = []\n",
    "for c in test.index:\n",
    "    test_aligned = get_aligned(test_real[c],test.sent[c])\n",
    "    aligned_text_list.append(test_aligned)\n",
    "test_aligned = pd.DataFrame({'sentence':aligned_text_list})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_aligned.to_csv(SAVE_ROOT_PATH+'test_aligned.tsv',sep='\\t',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Bert Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = df['Full Text'], df.outcome\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y , test_size = 0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from simpletransformers.classification import ClassificationModel\n",
    "from transformers import logging\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "train_args = {\n",
    "    'reprocess_input_data' : True,\n",
    "    'fp16':False,\n",
    "    'num_train_epochs':4,\n",
    "    'cuda_device':5,\n",
    "    'overwrite_output_dir':True\n",
    "    \n",
    "}\n",
    "\n",
    "model = ClassificationModel(\n",
    "                            \"bert\",\"wietsedv/bert-base-dutch-cased\",\n",
    "                            num_labels=2,args=train_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df[['Full Text','outcome']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check for vocabulary coverage\n",
    "\n",
    "df2 = df[['Full Text','outcome']].copy()\n",
    "# Getting the entire text \n",
    "full_text = ''\n",
    "for i in df2['Full Text']:\n",
    "    full_text = full_text+i + ' '\n",
    "    \n",
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "# Tokenized words from document\n",
    "tokenized_words =  [j for i in sent_tokenize(full_text) for j in word_tokenize(i)]\n",
    "tokenized_words_set = set(tokenized_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(df2 ,test_size = 0.3,random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train_model(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result, model_outputs, wrong_predictions = model.eval_model(test_df,accuracy=accuracy_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result ,model_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Commands to run for feature importance retrieval (using code of Lai et al. (2019))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "python bert_att_weight_retrieval.py --data_dir data/court_cases --bert_model data/court_cases/court_cases_model/ --task_name sst-2 --output_dir /data/court_cases/output --do_eval --max_seq_length 500 --eval_batch_size 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "python bert_lime.py --data_dir data/court_cases --bert_model data/court_cases/court_cases_model/ --task_name sst-2 --output_dir /data/court_cases/output --do_eval --max_seq_length 500 --eval_batch_size 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "python bert_shap.py --data_dir data/court_cases --bert_model data/court_cases/court_cases_model/ --task_name sst-2 --output_dir /data/court_cases/output --do_eval --max_seq_length 500 --eval_batch_size 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
