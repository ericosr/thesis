{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import time\n",
    "import requests\n",
    "import xmltodict\n",
    "import ast \n",
    "from nltk.tokenize import sent_tokenize\n",
    "from IPython.display import clear_output\n",
    "import googletrans\n",
    "from googletrans import Translator\n",
    "import math\n",
    "import sklearn.ensemble\n",
    "from sklearn.metrics import confusion_matrix,classification_report,accuracy_score\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "%matplotlib inline\n",
    "import html,nltk\n",
    "from nltk.corpus import wordnet \n",
    "from collections import Counter \n",
    "from string import digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"df_outcome_procedure_instantie.csv\", lineterminator='\\n', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_to_label = df.sample(300)\n",
    "# sample_to_label = df.to_pickle('sample_to_label.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_to_label = pd.read_pickle('sample_to_label.pickle')\n",
    "list_ECLI = sample_to_label.case.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_dataframe = pd.DataFrame({'ECLI': list_ECLI, 'sentences' : np.nan})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentences(ECLI):\n",
    "    content = df[df.case == ECLI].overwegingen.item()\n",
    "    return sent_tokenize(content) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_file(sentences, ECLI):\n",
    "    df = pd.DataFrame({'sentence':sentences, 'reveals_decision':np.nan})\n",
    "    name_of_file = 'labeled_sentences_{}.pickle'.format(ECLI)\n",
    "    df.to_pickle(name_of_file)\n",
    "    return name_of_file\n",
    "\n",
    "    \n",
    "def manually_label(pickle_file):\n",
    "    translator = Translator()\n",
    "    print('Does this sentence reveal the decision? Type 1 if yes. \\n')\n",
    "    df = pd.read_pickle(pickle_file)\n",
    "    for index, row in df[::-1].iterrows():\n",
    "        if pd.isnull(row.reveals_decision):\n",
    "            print(row.sentence)\n",
    "            result = translator.translate(row.sentence)\n",
    "            print(\"\\n\")\n",
    "            print(\"The translation is: \")\n",
    "            print(result.text)\n",
    "            reveals_decision = input()\n",
    "            if reveals_decision == '1':\n",
    "                df.loc[index, 'reveals_decision'] = 1\n",
    "            if reveals_decision == '':\n",
    "                df.loc[index, 'reveals_decision'] = 0\n",
    "            if reveals_decision == '0':\n",
    "                df.loc[index, 'reveals_decision'] = 0\n",
    "                clear_output()\n",
    "                df.to_pickle(pickle_file)\n",
    "                break\n",
    "            clear_output()\n",
    "            df.to_pickle(pickle_file)\n",
    "    \n",
    "    #append_df(pickle_file, 'labeled_sentences_ECLI:NL:RVS:2015:417.pickle')   \n",
    "    #append_df(pickle_file, 'labeled_sentences_ECLI:NL:RBSGR:2003:AH8572.pickle')   \n",
    "    #append_df(pickle_file, 'labeled_sentences_ECLI:NL:RBSGR:2010:BO1705.pickle')\n",
    "    \n",
    "\n",
    "    print('No more labels to classify!')\n",
    "\n",
    "\n",
    "def label(df):\n",
    "    for index,row in df.iterrows():\n",
    "        print(\"We are labelling: {}\".format(index))\n",
    "        print(\"https://uitspraken.rechtspraak.nl/inziendocument?id={}\".format(row.ECLI))\n",
    "        print(\"The ECLI of the file we are labelling is: {}\".format(row.ECLI))\n",
    "        name_of_file = create_file(row.sentences, row.ECLI)\n",
    "        manually_label(name_of_file)\n",
    "        \n",
    "        \n",
    "def append_df(new_file, df_pickle):\n",
    "    df1 = pd.read_pickle(df_pickle)\n",
    "    df2 = pd.read_pickle(new_file)\n",
    "    df1 = df1.append(df2)\n",
    "    df1 = df1.reset_index(drop=True)\n",
    "    \n",
    "    df1.to_pickle(df_pickle)\n",
    "\n",
    "def reset_labels(df_pickle):\n",
    "    df = pd.read_pickle(df_pickle)\n",
    "    df['label'] = np.nan\n",
    "    df.to_pickle(df_pickle)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training model and removing decision related sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"final_dataset_november.csv\", lineterminator='\\n', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_pickle('labeled_sentences_ECLI:NL:RVS:2015:417.pickle')\n",
    "df2 = pd.read_pickle('labeled_sentences_ECLI:NL:RBSGR:2003:AH8572.pickle')\n",
    "df3 = pd.read_pickle('labeled_sentences_ECLI:NL:RBSGR:2010:BO1705.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merging the dataframes together\n",
    "labeled_sentences_df = df1.append(df2)\n",
    "labeled_sentences_df = labeled_sentences_df.append(df3)\n",
    "labeled_sentences_df = labeled_sentences_df.fillna(0)\n",
    "labeled_sentences_df = labeled_sentences_df.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def undersample(df, target_col, r=1):\n",
    "    falses = df[target_col].value_counts()[0]\n",
    "    trues = df[target_col].value_counts()[1]\n",
    "    relation = float(trues)/float(falses)\n",
    "    if trues >= r*falses:\n",
    "        df_drop = df[df[target_col] == True]\n",
    "        drop_size = int(math.fabs(int((relation - r) * (falses))))\n",
    "    else: \n",
    "        df_drop = df[df[target_col] == False]\n",
    "        drop_size = int(math.fabs(int((r-relation) * (falses))))\n",
    "    df_drop = df_drop.sample(drop_size)\n",
    "    df = df.drop(labels=df_drop.index, axis=0)\n",
    "    return df\n",
    "\n",
    "def text_cleaning(text, escape_list=[], stop=[]):\n",
    "    \"\"\"\n",
    "    Text cleaning function:\n",
    "    \"\"\"\n",
    "    text=text.lower()\n",
    "    StopWords = list(set(stopwords.words('dutch')))\n",
    "    custom_stop = StopWords + stop\n",
    "    text = html.unescape(text)\n",
    "    text = re.sub('[^A-Za-z]+', ' ', text)\n",
    "    text=text.replace('/',' ').replace('?',' ').replace(',',' ').replace('\\'',' ')\n",
    "    tokenz=nltk.word_tokenize(text)\n",
    "    tokenz=([token for token in tokenz if token not in custom_stop]) \n",
    "    return ' '.join(tokenz)\n",
    "\n",
    "\n",
    "def do_cross_validation(models,features,labels,CV=5):\n",
    "    cv_df = pd.DataFrame(index=range(CV * len(models)))\n",
    "    entries = []\n",
    "    for model in models:\n",
    "        model_name = model.__class__.__name__\n",
    "        accuracies = cross_val_score(model, features, labels, scoring='accuracy', cv=CV,n_jobs=-1)\n",
    "        for fold_idx, accuracy in enumerate(accuracies):\n",
    "            entries.append((model_name, fold_idx, accuracy))\n",
    "    cv_df = pd.DataFrame(entries, columns=['model_name', 'fold_idx', 'accuracy'])\n",
    "    plot_cv_result(cv_df)\n",
    "    return cv_df\n",
    "\n",
    "def plot_cv_result(cv_df):\n",
    "    sns.boxplot(x='model_name', y='accuracy', data=cv_df)\n",
    "    sns.stripplot(x='model_name', y='accuracy', data=cv_df, \n",
    "                  size=8, jitter=True, edgecolor=\"gray\", linewidth=2)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def get_prediction(model,X_train,y_train,X_test,y_test):\n",
    "    y_pred_trn = model.predict(X_train)\n",
    "    conf_mat = confusion_matrix(y_train, y_pred_trn)\n",
    "    print(f'Accuracy for Training Set is  : {accuracy_score(y_train, y_pred_trn)}')\n",
    "    print(f'Confusion Matrix for Training Set :\\n {conf_mat} \\n\\n Classification Report for Training Set: \\n')\n",
    "    print(classification_report(y_train, y_pred_trn))\n",
    "    print('--'*50)\n",
    "    y_pred_tst = model.predict(X_test)\n",
    "    conf_mat_tst = confusion_matrix(y_test, y_pred_tst)\n",
    "    print(f'Accuracy for Testing Set is  : {accuracy_score(y_test, y_pred_tst)}')\n",
    "    print(f'Confusion Matrix for Testing Set :\\n {conf_mat_tst} \\n\\n Classification Report for Testing Set: /n')\n",
    "    print(classification_report(y_test, y_pred_tst))\n",
    "\n",
    "    \n",
    "def remove_decision_sentences(text, pipeline):\n",
    "    keep = []\n",
    "    text_sentences_list = sent_tokenize(text)\n",
    "    \n",
    "    for sentence in text_sentences_list:\n",
    "        if pipeline.predict(pd.Series(sentence)) == 0:\n",
    "            keep.append(sentence)\n",
    "\n",
    "    return ' '.join(keep)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "undersampled_df = undersample(labeled_sentences_df, 'reveals_decision')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "undersampled_df.reveals_decision.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "undersampled_df['sentence'] = undersampled_df['sentence'].apply(text_cleaning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    RandomForestClassifier(n_estimators=200, max_depth=3, random_state=0),\n",
    "    LinearSVC(),\n",
    "    MultinomialNB(),\n",
    "    LogisticRegression(random_state=0),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(rc={'figure.figsize':(16,6)})\n",
    "\n",
    "tfidf = TfidfVectorizer(sublinear_tf=True, min_df=5, norm='l2',ngram_range=(1,2))\n",
    "features = tfidf.fit_transform(undersampled_df['sentence'])\n",
    "labels = undersampled_df.reveals_decision\n",
    "print(features.shape)\n",
    "\n",
    "\n",
    "cv_df_tfidf = do_cross_validation(models,features,labels,CV=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(sublinear_tf=True, min_df=5, norm='l2',ngram_range=(1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_df_tfidf.groupby('model_name')['accuracy'].mean().to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(undersampled_df['sentence'], undersampled_df.reveals_decision, test_size=0.3, random_state=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(sublinear_tf=True, min_df=5, norm='l2',ngram_range=(1,2))\n",
    "pipeline = make_pipeline(tfidf,CalibratedClassifierCV(LinearSVC(),method='isotonic'))\n",
    "pipeline.fit(X_train,y_train)\n",
    "get_prediction(pipeline,X_train,y_train,X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.overwegingen = df.overwegingen.apply(remove_decision_sentences, pipeline = pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('alien_cases_dataset.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
